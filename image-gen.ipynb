{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D8Bt1ww60c9P",
        "outputId": "44dd46be-adec-4bc2-8669-c0f57c64b103"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m40.5/40.5 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m48.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m40.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hâœ… Environment Setup Complete!\n"
          ]
        }
      ],
      "source": [
        "# 1. Install required libraries\n",
        "!pip install -q segment-anything-py\n",
        "!pip install -q scikit-image\n",
        "\n",
        "# 2. Create directories for models and output\n",
        "!mkdir -p /content/sam_weights\n",
        "!mkdir -p /content/output\n",
        "\n",
        "# 3. Download the Segment-Anything-Model (SAM) weights\n",
        "!wget -q https://dl.fbaipublicfiles.com/segment_anything/sam_vit_l_0b3195.pth -O /content/sam_weights/sam_vit_l_0b3195.pth\n",
        "\n",
        "print(\"âœ… Environment Setup Complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import requests\n",
        "import io\n",
        "import base64\n",
        "from skimage import io as skio\n",
        "from segment_anything import sam_model_registry, SamPredictor\n",
        "from IPython.display import display\n",
        "\n",
        "# ===============================\n",
        "# Load SAM Model\n",
        "# ===============================\n",
        "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "try:\n",
        "    sam_l = sam_model_registry['vit_l'](checkpoint='/content/sam_weights/sam_vit_l_0b3195.pth').to(device=DEVICE)\n",
        "    print(\"âœ… SAM Model Loaded\")\n",
        "except Exception as e:\n",
        "    print(f\"âŒ Error loading SAM model:  {e}\")\n",
        "\n",
        "# ===============================\n",
        "# Helper Functions\n",
        "# ===============================\n",
        "def call_sd_api(api_url, endpoint, payload):\n",
        "    try:\n",
        "        response = requests.post(url=f'{api_url}/{endpoint}', json=payload, timeout=600)\n",
        "        response.raise_for_status()\n",
        "        return response.json()\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"âŒ API request failed:  {e}\")\n",
        "        return None\n",
        "\n",
        "def decode_and_resize(image_data, width, height):\n",
        "    image = Image.open(io.BytesIO(base64.b64decode(image_data.split(\",\", 1)[0])))\n",
        "    image = image.resize((width, height))\n",
        "    return np.array(image)\n",
        "\n",
        "# ===============================\n",
        "# Image Processing Classes\n",
        "# ===============================\n",
        "class BackRemover:\n",
        "    def __init__(self, image):\n",
        "        self.image = image\n",
        "\n",
        "    def image_preprocessor(self):\n",
        "        predictor = SamPredictor(sam_l)\n",
        "        predictor.set_image(self.image)\n",
        "        h, w, _ = self.image.shape\n",
        "        input_box = np.array([w * 0.05, h * 0.05, w * 0.95, h * 0.95])\n",
        "        masks, _, _ = predictor.predict(box=input_box, multimask_output=False)\n",
        "        mask = masks[0]\n",
        "        alpha_channel = np.where(mask, 255, 0).astype(np.uint8)\n",
        "        bgr_image = cv2.cvtColor(self.image, cv2.COLOR_RGB2BGR)\n",
        "        bgra_image = cv2.cvtColor(bgr_image, cv2.COLOR_BGR2BGRA)\n",
        "        bgra_image[:, :, 3] = alpha_channel\n",
        "        x, y, w, h = cv2.boundingRect(alpha_channel)\n",
        "        return bgra_image[y:y+h, x:x+w]\n",
        "\n",
        "class AdGenerator:\n",
        "    def __init__(self, product_image_path, api_url):\n",
        "        self.product_image_path = product_image_path\n",
        "        self.api_url = api_url\n",
        "        self.transparent_product = self._prepare_product_image()\n",
        "\n",
        "    def _prepare_product_image(self):\n",
        "        print(\"ï¸Preparing product image...\")\n",
        "        image = skio.imread(self.product_image_path)\n",
        "        if image.shape[2] == 4:\n",
        "            image = cv2.cvtColor(image, cv2.COLOR_RGBA2RGB)\n",
        "        remover = BackRemover(image)\n",
        "        return remover.image_preprocessor()\n",
        "\n",
        "    def generate(self, background_prompt, final_blend_prompt, negative_prompt, width=910, height=512):\n",
        "        if self.transparent_product is None: return None\n",
        "\n",
        "        print(\"Step 1: Generating background...\")\n",
        "        payload = {\n",
        "            \"prompt\": background_prompt,\n",
        "            \"negative_prompt\": negative_prompt,\n",
        "            \"steps\": 25,\n",
        "            \"width\": width,\n",
        "            \"height\": height,\n",
        "            \"seed\": -1\n",
        "        }\n",
        "        r = call_sd_api(self.api_url, 'sdapi/v1/txt2img', payload)\n",
        "        if not r or 'images' not in r: return None\n",
        "        background = decode_and_resize(r['images'][0], width, height)\n",
        "        background = cv2.cvtColor(background, cv2.COLOR_RGB2BGR)\n",
        "\n",
        "        print(\"Step 2: Compositing product with enhancements...\")\n",
        "        max_prod_width, max_prod_height = int(width * 0.7), int(height * 0.8)\n",
        "        h_orig, w_orig, _ = self.transparent_product.shape\n",
        "        ratio = min(max_prod_width / w_orig, max_prod_height / h_orig)\n",
        "        new_w, new_h = int(w_orig * ratio), int(h_orig * ratio)\n",
        "        product_resized = cv2.resize(self.transparent_product, (new_w, new_h), interpolation=cv2.INTER_LANCZOS4)\n",
        "\n",
        "        product_resized[:, :, :3] = cv2.addWeighted(\n",
        "            product_resized[:, :, :3], 0.95,\n",
        "            np.full_like(product_resized[:, :, :3], (10, 20, 0), dtype=np.uint8), 0.05, 0\n",
        "        )\n",
        "\n",
        "        h_prod, w_prod, _ = product_resized.shape\n",
        "        h_bg, w_bg, _ = background.shape\n",
        "        y_offset, x_offset = (h_bg - h_prod) // 2, (w_bg - w_prod) // 2\n",
        "        overlay = background.copy()\n",
        "\n",
        "        # --- SHADOW FIX: Made shadow softer and more transparent ---\n",
        "        shadow_shape = np.zeros_like(overlay)\n",
        "        shadow_shape[y_offset:y_offset+h_prod, x_offset:x_offset+w_prod] = (50, 50, 50)\n",
        "        blurred_shadow_shape = cv2.GaussianBlur(shadow_shape, (75, 75), 0)\n",
        "\n",
        "        shadow_offset_x, shadow_offset_y = 15, 15\n",
        "        shadow_y_start, shadow_x_start = y_offset + shadow_offset_y, x_offset + shadow_offset_x\n",
        "        shadow_y_end, shadow_x_end = min(shadow_y_start + h_prod, h_bg), min(shadow_x_start + w_prod, w_bg)\n",
        "        shadow_roi = overlay[shadow_y_start:shadow_y_end, shadow_x_start:shadow_x_end]\n",
        "        blurred_shadow_roi = blurred_shadow_shape[y_offset:y_offset+(shadow_y_end - shadow_y_start),\n",
        "                                                  x_offset:x_offset+(shadow_x_end - shadow_x_start)]\n",
        "\n",
        "        alpha_mask = cv2.resize(product_resized[:, :, 3], (shadow_roi.shape[1], shadow_roi.shape[0])) / 255.0\n",
        "        alpha_mask = np.expand_dims(alpha_mask, axis=2)\n",
        "\n",
        "        overlay[shadow_y_start:shadow_y_end, shadow_x_start:shadow_x_end] = \\\n",
        "            (1 - alpha_mask * 0.3) * shadow_roi + (alpha_mask * 0.3) * blurred_shadow_roi\n",
        "        # --- END OF SHADOW FIX ---\n",
        "\n",
        "        product_mask = product_resized[:, :, 3] / 255.0\n",
        "        for c in range(3):\n",
        "            overlay[y_offset:y_offset+h_prod, x_offset:x_offset+w_prod, c] = (\n",
        "                (1.0 - product_mask) * overlay[y_offset:y_offset+h_prod, x_offset:x_offset+w_prod, c] +\n",
        "                product_mask * product_resized[:, :, c]\n",
        "            )\n",
        "\n",
        "        print(\"Step 3: Final polish using img2img...\")\n",
        "        outpaint_mask = np.zeros((h_bg, w_bg), dtype=np.uint8)\n",
        "        outpaint_mask[y_offset:y_offset+h_prod, x_offset:x_offset+w_prod] = 255\n",
        "        _, buffer_overlay = cv2.imencode('.png', overlay)\n",
        "        overlay_b64 = base64.b64encode(buffer_overlay).decode('utf-8')\n",
        "        _, buffer_mask = cv2.imencode('.png', outpaint_mask)\n",
        "        mask_b64 = base64.b64encode(buffer_mask).decode('utf-8')\n",
        "\n",
        "        payload = {\n",
        "            \"init_images\": [overlay_b64],\n",
        "            \"mask\": mask_b64,\n",
        "            \"inpainting_mask_invert\": 1,\n",
        "            \"prompt\": final_blend_prompt,\n",
        "            \"negative_prompt\": negative_prompt,\n",
        "            \"steps\": 50,\n",
        "            \"denoising_strength\": 0.35,\n",
        "            \"width\": width,\n",
        "            \"height\": height,\n",
        "            \"seed\": -1,\n",
        "            \"inpainting_fill\": 1\n",
        "        }\n",
        "        r = call_sd_api(self.api_url, 'sdapi/v1/img2img', payload)\n",
        "        if not r or 'images' not in r: return None\n",
        "        final_image_np = decode_and_resize(r['images'][0], width, height)\n",
        "        return Image.fromarray(final_image_np)\n",
        "\n",
        "# ===============================\n",
        "# SCRIPT ENTRY POINT\n",
        "# ===============================\n",
        "if __name__ == '__main__':\n",
        "    api_url = \"https://c3a8f831a5dfb3e739.gradio.live\"\n",
        "    product_image_path = \"/content/51zuwnYnMWL._SL1000_.jpg\"\n",
        "    output_path = \"/content/output/generated_ad_final.png\"\n",
        "\n",
        "    background_prompt = (\n",
        "        \"A rustic wooden kitchen counter with morning sunlight streaming in, a fresh jar of tomato sauce at the center, \"\n",
        "        \"surrounded by ripe tomatoes, garlic cloves, basil leaves, and a wooden spoon. The background includes a vintage Italian kitchen setup. \"\n",
        "        \"Professional food photography, high contrast, shallow depth of field, realistic textures, 8k, ultra high detail, warm tones\"\n",
        "    )\n",
        "\n",
        "    final_blend_prompt = (\n",
        "        background_prompt +\n",
        "        \", seamless blend between product and background, natural shadow under the bottle, realistic light reflection on the bottle, photo-realistic, smooth textures\"\n",
        "    )\n",
        "\n",
        "    negative_prompt = \"blurry, dark, grainy, text, watermark, people, hands, clutter, modern, plastic, cartoon, anime\"\n",
        "\n",
        "    if \"gradio.live\" not in api_url and \"127.0.0.1\" not in api_url:\n",
        "        print(\"âŒ ERROR: Please set a valid public `api_url`.\")\n",
        "    else:\n",
        "        print(\"ğŸš€ Starting Final Hackathon Ad Generation Pipeline...\")\n",
        "        generator = AdGenerator(product_image_path, api_url)\n",
        "        final_ad_image = generator.generate(background_prompt, final_blend_prompt, negative_prompt)\n",
        "\n",
        "        if final_ad_image:\n",
        "            final_ad_image.save(output_path)\n",
        "            print(f\"âœ… Success! Image saved to  {output_path}\")\n",
        "            display(final_ad_image)\n",
        "        else:\n",
        "            print(\"âŒ Ad generation failed.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F-suRpS_0hr4",
        "outputId": "76d2d0e9-6b66-44e4-8d48-c674092eb428"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… SAM Model Loaded\n",
            "ğŸš€ Starting Final Hackathon Ad Generation Pipeline...\n",
            "ï¸Preparing product image...\n",
            "Step 1: Generating background...\n",
            "âŒ API request failed:  404 Client Error: Not Found for url: https://c3a8f831a5dfb3e739.gradio.live/sdapi/v1/txt2img\n",
            "âŒ Ad generation failed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nY7YOE621WXM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}